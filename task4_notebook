{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":982,"sourceType":"datasetVersion","datasetId":483}],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_columns', None)\ndata = pd.read_csv('/kaggle/input/sms-spam-collection-dataset/spam.csv', encoding='latin1')\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data.iloc[:, :2]\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data.shape)\nprint(data.isnull().sum())\nprint('\\n')\ndata.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_entries = data.nunique()\nprint(unique_entries)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.rename(columns={'v1': 'target', 'v2': 'text'}, inplace=True)\ndata.sample(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\ndata['target'] = encoder.fit_transform(data['target'])\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data.isnull().sum())\nprint('\\n')\nprint(data.duplicated().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data.drop_duplicates(keep='first')\ndata.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. EDA","metadata":{}},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['target'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(encoder.classes_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt \nplt.pie(data['target'].value_counts(), labels=encoder.classes_, autopct = \"%0.2f\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nltk.download('punkt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['num_characters'] = data['text'].apply(len)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['num_words'] = data['text'].apply(lambda x: len(nltk.word_tokenize(x)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['num_sentences'] = data['text'].apply(lambda x: len(nltk.sent_tokenize(x)))\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[['num_characters', 'num_words', 'num_sentences']].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder.classes_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[data['target'] == 1][['num_characters', 'num_words', 'num_sentences']].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[data['target'] == 0][['num_characters', 'num_words', 'num_sentences']].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\n\n# Iterate over each specified column in the DataFrame\nfor column in ['num_characters', 'num_words', 'num_sentences']:\n    # Create a new figure for each column with specified size\n    plt.figure(figsize=(12,6))\n    \n    # Plot the histogram for target 0\n    sns.histplot(data[data['target'] == 0][column], color = 'blue', label = 'Target 0', kde = True)\n    \n    # Plot the histogram for target 1\n    sns.histplot(data[data['target'] == 1][column], color = 'red', label = 'Target 1', kde = True)\n    \n    # Add a title\n    plt.title('Histogram of ' + column)\n    \n    # Add a legend\n    plt.legend()\n    \n    # Display the plot\n    plt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(data.drop('text', axis = 1).corr(), annot = True)\ndata.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Data Preprocessing","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nimport string \nfrom collections import Counter\nps = PorterStemmer()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def transform_text(text):\n    text = text.lower()\n    text = nltk.word_tokenize(text)\n    y = []\n    for i in text:\n        if i.isalnum():\n            y.append(i)\n    text = y[:]\n    y.clear()\n    for i in text:\n        if i not in stopwords.words('english') and i not in string.punctuation:\n            y.append(i)\n    text = y[:]\n    y.clear()\n    \n    for i in text:\n        y.append(ps.stem(i))\n    return \" \".join(y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform_text('I loved the YT lectures on machine learning')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['transformed_text'] = data['text'].apply(transform_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from wordcloud import WordCloud\nwc = WordCloud(width=500, height=500, min_font_size=10, background_color='white')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spam_wc = wc.generate(data[data['target']==1]['transformed_text'].str.cat(sep=\" \"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(spam_wc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ham_wc = wc.generate(data[data['target']==0]['transformed_text'].str.cat(sep=\" \"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(ham_wc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spam_corpus = []\nfor msg in data[data['target']==1]['transformed_text'].tolist():\n    for word in msg.split():\n        spam_corpus.append(word)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(spam_corpus)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1 = pd.DataFrame(Counter(spam_corpus).most_common(30))\nsns.barplot(x=df1.iloc[:, 0], y=df1.iloc[:, 1])\nplt.xticks(rotation='vertical')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ham_corpus = []\nfor msg in data[data['target']==0]['transformed_text'].tolist():\n    for word in msg.split():\n        ham_corpus.append(word)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(ham_corpus)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1 = pd.DataFrame(Counter(ham_corpus).most_common(30))\nsns.barplot(x=df1.iloc[:, 0], y=df1.iloc[:, 1])\nplt.xticks(rotation='vertical')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4. Model Building","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\ncv = CountVectorizer()\ntfidf = TfidfVectorizer()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = cv.fit_transform(data['transformed_text']).toarray()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = data['target'].values\ny","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train , X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gnb = GaussianNB()\nmnb = MultinomialNB()\nbnb = BernoulliNB()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gnb.fit(X_train, y_train)\ny_pred1 = gnb.predict(X_test)\nprint(accuracy_score(y_test, y_pred1))\nprint(confusion_matrix(y_test, y_pred1))\nprint(precision_score(y_test, y_pred1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mnb.fit(X_train, y_train)\ny_pred1 = mnb.predict(X_test)\nprint(accuracy_score(y_test, y_pred1))\nprint(confusion_matrix(y_test, y_pred1))\nprint(precision_score(y_test, y_pred1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bnb.fit(X_train, y_train)\ny_pred1 = bnb.predict(X_test)\nprint(accuracy_score(y_test, y_pred1))\nprint(confusion_matrix(y_test, y_pred1))\nprint(precision_score(y_test, y_pred1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = tfidf.fit_transform(data['transformed_text']).toarray()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = data['target'].values\ny","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train , X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gnb = GaussianNB()\nmnb = MultinomialNB()\nbnb = BernoulliNB()\nlst = [gnb, mnb, bnb]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for classifier in lst:\n    print(classifier)\n    classifier.fit(X_train, y_train)\n    y_pred1 = classifier.predict(X_test)\n    print(accuracy_score(y_test, y_pred1))\n    print(confusion_matrix(y_test, y_pred1))\n    print(precision_score(y_test, y_pred1))\n    print('\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gnb.fit(X_train, y_train)\ny_pred1 = gnb.predict(X_test)\nprint(accuracy_score(y_test, y_pred1))\nprint(confusion_matrix(y_test, y_pred1))\nprint(precision_score(y_test, y_pred1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mnb.fit(X_train, y_train)\ny_pred1 = mnb.predict(X_test)\nprint(accuracy_score(y_test, y_pred1))\nprint(confusion_matrix(y_test, y_pred1))\nprint(precision_score(y_test, y_pred1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\n \nclassifier = {\n    \"Logistic Regression\": LogisticRegression(),\n#     \"Decision Tree Classifier\": DecisionTreeClassifier(),\n#     \"Random Forest Classifier\": RandomForestClassifier(),\n    \"Support Vector Classifier\": SVC(),\n#     \"K-Nearest Neighbors Classifier\": KNeighborsClassifier(),\n    \"Gaussian Naive Bayes\": GaussianNB(),\n    \"Multinomial Naive Bayes\": MultinomialNB(),\n    \"BernoulliNB Naive Bayes\": BernoulliNB(),\n    \"AdaBoost Classifier\": AdaBoostClassifier(),\n    \"Gradient Boosting Classifier\": GradientBoostingClassifier(),\n#     \"Bagging Classifier\": BaggingClassifier(),\n#     \"Extra Trees Classifier\": ExtraTreesClassifier(),\n#     \"Stochastic Gradient Descent Classifier\": SGDClassifier(),\n#     \"Voting Classifier\": VotingClassifier(estimators=[\n#         ('lr', LogisticRegression()),\n#         ('dt', DecisionTreeClassifier()),\n#         ('rf', RandomForestClassifier()),\n#         ('svc', SVC()),\n#         ('knn', KNeighborsClassifier())\n#     ], voting='hard')\n}\n\nfor name, clf in classifier.items():\n    print(f\"\\n=========={name}===========\")\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    \n    # Evaluation Metrics\n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n    print(f\"\\n Accuracy: {accuracy}\")\n    print(f\" Precision: {precision}\")\n    print(f\" Recall: {recall}\")\n    print(f\" F1 Score: {f1}\")\n    \n    # Confusion Matrix\n    print(\"\\n Confusion Matrix:\")\n    print(confusion_matrix(y_test, y_pred))\n    \n    # Classification Report\n    print(\"\\n Classification Report:\")\n    print(classification_report(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**BernouliNB Naive Bayes**","metadata":{}},{"cell_type":"code","source":"pip install joblib","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import joblib\n\n# Assuming 'clf' is your trained BernoulliNB Naive Bayes classifier\nbernoulli_nb_classifier = classifier['BernoulliNB Naive Bayes']\n\n# Save the model to a file\njoblib.dump(bernoulli_nb_classifier, 'bernoulli_nb_classifier.pkl')\njoblib.dump(tfidf, 'vectorizer.pkl')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\n# List all files in the current directory\nfiles = os.listdir('.')\nprint(\"Files in the current directory:\", files)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}